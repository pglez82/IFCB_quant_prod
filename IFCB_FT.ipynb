{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysuTmf6SE9KB"
   },
   "source": [
    "# Load the data\n",
    "We are going to finetune a resnet and extract features with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "Iv08LtzkFGee",
    "outputId": "90d7c4c1-d3fb-46b6-e4e1-612d3e424456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  year  \n",
      "0                   mix      Flagellate  2006  \n",
      "1           ciliate_mix         Ciliate  2006  \n",
      "2                   mix      Flagellate  2006  \n",
      "3                   mix      Flagellate  2006  \n",
      "4                   mix      Flagellate  2006  \n",
      "...                 ...             ...   ...  \n",
      "3457814  Leptocylindrus          Diatom  2014  \n",
      "3457815             mix      Flagellate  2014  \n",
      "3457816             mix      Flagellate  2014  \n",
      "3457817             mix      Flagellate  2014  \n",
      "3457818             mix      Flagellate  2014  \n",
      "\n",
      "[3457819 rows x 6 columns]\n",
      "Samples from 2009\n",
      "                       year\n",
      "Sample                     \n",
      "IFCB1_2009_001_001602  2009\n",
      "IFCB1_2009_001_003939  2009\n",
      "IFCB1_2009_006_201718  2009\n",
      "IFCB1_2009_006_203845  2009\n",
      "IFCB1_2009_006_210219  2009\n",
      "...                     ...\n",
      "IFCB1_2009_341_000206  2009\n",
      "IFCB1_2009_341_002518  2009\n",
      "IFCB1_2009_341_004829  2009\n",
      "IFCB1_2009_362_002638  2009\n",
      "IFCB1_2009_362_004552  2009\n",
      "\n",
      "[171 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "data['year'] = data['Sample'].str[6:10].astype(str) #Compute the year\n",
    "samples=data.groupby('Sample').first()\n",
    "samples=samples[['year']]\n",
    "print(data)\n",
    "print(\"Samples from 2009\")\n",
    "print(samples[samples['year']=='2009'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which years we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing image paths...\n",
      "Done\n",
      "col_0   count\n",
      "year         \n",
      "2006   131002\n",
      "2007   273080\n",
      "2008   427308\n",
      "2009   732398\n",
      "2010   327996\n",
      "2011   419692\n",
      "2012   394766\n",
      "2013   422255\n",
      "2014   329322\n",
      "Training data already there... Doing nothing\n",
      "Validation data already there... Doing nothing\n",
      "Production data already there... Doing nothing\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "yearstraining = ['2006','2007','2008'] #Years to consider as training\n",
    "samplesvalidation = ['IFCB1_2009_001_001602'] #Samples to consider for validation\n",
    "sampleproduction = 'IFCB1_2009_001_001602' #Sample to consider for production. This will be used in other notebook\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "productionfolder = \"production\" #This is where images are going when we have no labels and we want to predict them\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "#Check data by year\n",
    "print(pd.crosstab(index=data['year'],columns='count'))\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set... Using years:\")\n",
    "  print(yearstraining)\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: copyfile(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set... Using samples:\")\n",
    "  print(samplesvalidation)\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['Sample'].isin(samplesvalidation)].progress_apply(lambda row: copyfile(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(productionfolder):\n",
    "  print(\"Create folder structure for production ... Using sample:\")\n",
    "  print(sampleproduction)\n",
    "  os.mkdir(productionfolder)\n",
    "  print(\"Done.\\nMoving images to the same folder...\")\n",
    "  data[data['Sample']==sampleproduction].progress_apply(lambda row: copyfile(row['path'],os.path.join(productionfolder,os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Production data already there... Doing nothing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "np.random.seed(0)\n",
    "\n",
    "num_workers = 4 # @param\n",
    "batch_size = 256 # @param \n",
    "batch_size_val = 512 # @param \n",
    "\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "\n",
    "is_trained=False #Will take true if the network is already trained\n",
    "model_save_path=\"model.pt\" #Where to save the model once trained\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.CenterCrop(size=224),\n",
    "  T.ToTensor(),\n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "  \n",
    "\n",
    "#Define data loader\n",
    "num_classes = 51\n",
    "train_dset = ImageFolder(trainingfolder, transform=train_transform)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
    "val_dset = ImageFolder(validationfolder, transform=val_transform)\n",
    "val_loader = DataLoader(val_dset,batch_size=batch_size_val,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Define how to load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem. In the case that model_trained_path is true, that means that we have already trained the network so we load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def load_network():\n",
    "  global is_trained\n",
    "  model = torchvision.models.resnet34(pretrained=True)\n",
    "  print(\"Adjusting the CNN for %s classes\" % num_classes)\n",
    "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "  #Define loss function\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  if os.path.isfile(model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    is_trained=True\n",
    "  model = model.to(device) #Send model to gpu\n",
    "  return model,loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define finetuning util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "  \"\"\"\n",
    "  Train the model for one epoch.\n",
    "  \"\"\"\n",
    "  loss_epoch = 0 \n",
    "  start_time = time.time()\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run the model forward to compute scores and loss.\n",
    "    scores = model(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "    loss_epoch = loss_epoch + loss.item()\n",
    "    # Run the model backward and take a step using the optimizer.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50== 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "  return loss_epoch\n",
    "\n",
    "def make_preds(model, loader, device):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    for x, y in loader:\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      # Run the model forward, and compare the argmax score with the ground-truth\n",
    "      # category.\n",
    "      output = model(x)\n",
    "      predicted = output.argmax(1)\n",
    "      prob = nnf.softmax(output, dim=1)\n",
    "      y_probs.extend(prob.cpu().detach().numpy())\n",
    "      y_true.extend(y.cpu().numpy())\n",
    "      y_pred.extend(predicted.cpu().numpy())\n",
    "  return y_true,y_pred,y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Define the finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finetune(model,loss_fn,train_loader,device):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "  #First phase of finetuning\n",
    "  for epoch in range(num_epochs_ft1):\n",
    "    # Run an epoch over the training data.\n",
    "    print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "    loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "    # Check accuracy on the train and val sets.\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ft1}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "  #Allow updating all the weights in the second phase\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "  #Lower learning rate this time\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "  # Train the entire model for a few more epochs, checking accuracy on the\n",
    "  # train sets after each epoch.\n",
    "  for epoch in range(num_epochs_ft2):\n",
    "    print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "    loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ft2}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "    \n",
    "  torch.save(model.state_dict(), \"model.pt\")\n",
    "  print(\"Fine tune done and model saved.\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute everything and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process...\n",
      "Adjusting the CNN for 51 classes\n",
      "Model was trained already\n",
      "Performing final validation in test examples...\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   Asterionellopsis       0.75      0.30      0.43        10\n",
      "           Ceratium       1.00      1.00      1.00         1\n",
      "        Chaetoceros       0.62      0.57      0.59        14\n",
      "          Corethron       1.00      1.00      1.00         3\n",
      "      Coscinodiscus       0.50      0.50      0.50         2\n",
      "      Cylindrotheca       1.00      0.90      0.95        20\n",
      "   DactFragCerataul       0.00      0.00      0.00         0\n",
      "      Dactyliosolen       0.00      0.00      0.00         0\n",
      "          Dictyocha       0.00      0.00      0.00         0\n",
      "            Ditylum       1.00      1.00      1.00         7\n",
      "          Gonyaulax       1.00      1.00      1.00        18\n",
      "          Guinardia       0.86      0.95      0.90        20\n",
      " Guinardia_flaccida       1.00      1.00      1.00         1\n",
      "  Guinardia_striata       1.00      1.00      1.00         1\n",
      "         Gyrodinium       1.00      1.00      1.00         2\n",
      "     Leptocylindrus       0.93      0.93      0.93       166\n",
      "         Myrionecta       0.00      0.00      0.00         0\n",
      "        Pleurosigma       0.00      0.00      0.00         0\n",
      "       Prorocentrum       0.00      0.00      0.00         1\n",
      "    Pseudonitzschia       0.00      0.00      0.00         2\n",
      "       Rhizosolenia       0.00      0.00      0.00         1\n",
      "        Skeletonema       0.50      1.00      0.67         4\n",
      "      Thalassiosira       0.67      0.90      0.77        20\n",
      "Thalassiosira_dirty       0.80      0.50      0.62         8\n",
      "        ciliate_mix       1.00      1.00      1.00         6\n",
      "           detritus       0.91      0.95      0.93       451\n",
      "             dino30       0.25      0.12      0.17         8\n",
      "                mix       0.98      0.96      0.97      1190\n",
      "      mix_elongated       0.00      0.00      0.00         6\n",
      "                 na       0.67      0.67      0.67         3\n",
      "            pennate       0.00      0.00      0.00         1\n",
      "          tintinnid       1.00      1.00      1.00         1\n",
      "\n",
      "           accuracy                           0.94      1967\n",
      "          macro avg       0.58      0.57      0.57      1967\n",
      "       weighted avg       0.94      0.94      0.94      1967\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Starting process...')\n",
    "\n",
    "model,loss_fn = load_network()\n",
    "\n",
    "if not is_trained:\n",
    "  finetune(model,loss_fn,train_loader,device)\n",
    "else:\n",
    "  print(\"Model was trained already\")\n",
    "    \n",
    "print(\"Performing final validation in test examples...\")\n",
    "y_true,y_pred,y_probs = make_preds(model, val_loader, device)\n",
    "labelswithexamples=np.union1d(np.unique(y_true),np.unique(y_pred))\n",
    "labelswithexamples_names = np.array(list(val_dset.class_to_idx.keys()))[labelswithexamples]\n",
    "print(classification_report(y_true, y_pred,target_names=labelswithexamples_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing things for quantification\n",
    "For quantification we need the training set classified with probabilities. As this takes a long time is better to do it here and save the results to a file. We can use this later to fit the quantifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results to check that everything is working...\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   Asterionellopsis       0.72      0.59      0.65       311\n",
      "        Cerataulina       0.70      0.76      0.73      4620\n",
      "           Ceratium       0.82      0.69      0.75       104\n",
      "        Chaetoceros       0.70      0.69      0.70     10445\n",
      "          Corethron       0.88      0.86      0.87       774\n",
      "      Coscinodiscus       0.72      0.60      0.65        85\n",
      "      Cylindrotheca       0.80      0.84      0.82      2744\n",
      "   DactFragCerataul       0.62      0.56      0.59      2840\n",
      "      Dactyliosolen       0.79      0.85      0.82      1538\n",
      "          Dictyocha       0.93      0.81      0.86       392\n",
      "          Dinobryon       0.86      0.84      0.85       703\n",
      "         Dinophysis       0.92      0.21      0.34        57\n",
      "            Ditylum       0.85      0.81      0.83       236\n",
      "           Ephemera       0.84      0.77      0.80       115\n",
      "           Eucampia       0.75      0.60      0.67       139\n",
      "            Euglena       0.56      0.25      0.35       260\n",
      "          Gonyaulax       0.94      0.97      0.96       484\n",
      "          Guinardia       0.85      0.93      0.89      6933\n",
      " Guinardia_flaccida       0.83      0.82      0.83       191\n",
      "  Guinardia_striata       0.92      0.59      0.72       204\n",
      "         Gyrodinium       0.64      0.56      0.59       457\n",
      "          Hemiaulus       0.00      0.00      0.00         4\n",
      "             Laboea       0.79      0.75      0.77        51\n",
      "           Lauderia       0.47      0.50      0.48        14\n",
      "     Leptocylindrus       0.85      0.91      0.88     40239\n",
      "         Licmophora       0.50      0.33      0.39        46\n",
      "         Myrionecta       0.83      0.72      0.77       762\n",
      "          Odontella       0.89      0.62      0.73        13\n",
      "            Paralia       0.77      0.67      0.72       145\n",
      "        Phaeocystis       0.86      0.55      0.67        33\n",
      "        Pleurosigma       0.91      0.85      0.88       332\n",
      "       Prorocentrum       0.71      0.74      0.73       539\n",
      "    Pseudonitzschia       0.76      0.54      0.63      1082\n",
      "        Pyramimonas       0.85      0.60      0.70       131\n",
      "       Rhizosolenia       0.84      0.91      0.87      6293\n",
      "        Skeletonema       0.79      0.70      0.74      3326\n",
      "      Stephanopyxis       1.00      0.40      0.57         5\n",
      "      Thalassionema       0.89      0.85      0.87      1046\n",
      "      Thalassiosira       0.63      0.64      0.63      4388\n",
      "Thalassiosira_dirty       0.80      0.55      0.65       808\n",
      "                bad       0.84      0.73      0.78      2424\n",
      "        ciliate_mix       0.73      0.82      0.77      2834\n",
      "  clusterflagellate       0.84      0.41      0.55       127\n",
      "           detritus       0.74      0.72      0.73     93770\n",
      "             dino30       0.51      0.53      0.52      9754\n",
      "    kiteflagellates       0.67      0.62      0.64        63\n",
      "                mix       0.94      0.95      0.95    611401\n",
      "      mix_elongated       0.59      0.36      0.44     14626\n",
      "                 na       0.57      0.38      0.45      1801\n",
      "            pennate       0.65      0.45      0.53      1555\n",
      "          tintinnid       0.72      0.68      0.70       146\n",
      "\n",
      "           accuracy                           0.89    831390\n",
      "          macro avg       0.76      0.65      0.69    831390\n",
      "       weighted avg       0.89      0.89      0.89    831390\n",
      "\n",
      "Saving results to a csv...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-31401949b203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabelswithexamples_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving results to a csv...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/trainclassifications.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "y_true,y_pred,y_probs = make_preds(model, train_loader, device)\n",
    "print(\"Printing results to check that everything is working...\")\n",
    "labelswithexamples=np.union1d(np.unique(y_true),np.unique(y_pred))\n",
    "labelswithexamples_names = np.array(list(train_dset.class_to_idx.keys()))[labelswithexamples]\n",
    "#We save it to disk so we can use it later\n",
    "np.savetxt('results/classes.csv',labelswithexamples_names,fmt = '%s')\n",
    "print(classification_report(y_true, y_pred,target_names=labelswithexamples_names))\n",
    "print(\"Saving results to a csv...\")\n",
    "np.savetxt(\"results/traintrue.csv\",y_true,fmt='%d')\n",
    "np.savetxt(\"results/trainpred.csv\",y_pred,fmt='%d')\n",
    "np.savetxt(\"results/trainprobs.csv\", y_probs, delimiter=\",\",fmt='%f')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
