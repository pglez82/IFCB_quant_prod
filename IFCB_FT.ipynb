{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysuTmf6SE9KB"
   },
   "source": [
    "# Load the data\n",
    "We are going to finetune a resnet and extract features with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "Iv08LtzkFGee",
    "outputId": "90d7c4c1-d3fb-46b6-e4e1-612d3e424456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  \n",
      "0                   mix      Flagellate  \n",
      "1           ciliate_mix         Ciliate  \n",
      "2                   mix      Flagellate  \n",
      "3                   mix      Flagellate  \n",
      "4                   mix      Flagellate  \n",
      "...                 ...             ...  \n",
      "3457814  Leptocylindrus          Diatom  \n",
      "3457815             mix      Flagellate  \n",
      "3457816             mix      Flagellate  \n",
      "3457817             mix      Flagellate  \n",
      "3457818             mix      Flagellate  \n",
      "\n",
      "[3457819 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which years we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing image paths...\n",
      "Done\n",
      "col_0   count\n",
      "year         \n",
      "2006   131002\n",
      "2007   273080\n",
      "2008   427308\n",
      "2009   732398\n",
      "2010   327996\n",
      "2011   419692\n",
      "2012   394766\n",
      "2013   422255\n",
      "2014   329322\n",
      "Training data already there... Doing nothing\n",
      "Validation data already there... Doing nothing\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "yearstraining = ['2006','2007','2008'] #Years to consider as training\n",
    "yearsvalidation = ['2009']\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['year'] = data['Sample'].str[6:10].astype(str)\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "#Check data by year\n",
    "print(pd.crosstab(index=data['year'],columns='count'))\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set... Using years:\")\n",
    "  print(yearstraining)\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: copyfile(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set... Using years:\")\n",
    "  print(yearsvalidation)\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearsvalidation)].progress_apply(lambda row: copyfile(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "np.random.seed(0)\n",
    "\n",
    "num_workers = 4 # @param\n",
    "batch_size = 256 # @param \n",
    "batch_size_val = 128 # @param \n",
    "\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.CenterCrop(size=224),\n",
    "  T.ToTensor(),\n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "  \n",
    "\n",
    "#Define data loader\n",
    "num_classes = 51\n",
    "train_dset = ImageFolder(trainingfolder, transform=train_transform)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
    "val_dset = ImageFolder(validationfolder, transform=val_transform)\n",
    "val_loader = DataLoader(val_dset,batch_size=128,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot some information about training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "def plot_dataset_distribution(y,labels):\n",
    "  counts = np.bincount(y,minlength=len(labels))\n",
    "  plt.bar(labels, counts)\n",
    "  plt.xticks(rotation='vertical')\n",
    "\n",
    "#Print training set and its subsets\n",
    "figure(num=None, figsize=(15, 10),facecolor='w', edgecolor='k')\n",
    "labels = train_dset.class_to_idx.keys()\n",
    "y = np.array(train_dset.targets)\n",
    "plot_dataset_distribution(y,labels)\n",
    "y = np.array(np.array(train_dset.targets))\n",
    "plot_dataset_distribution(y,labels)\n",
    "plt.title('Years'+str(yearstraining))\n",
    "plt.suptitle('Classes distribution.')\n",
    "    \n",
    "#Print validation dataset\n",
    "fig=figure(num=None, figsize=(15, 10),facecolor='w', edgecolor='k')\n",
    "y = np.array(val_dset.targets)\n",
    "plot_dataset_distribution(y,labels)\n",
    "fig.suptitle('Years'+str(yearsvalidation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Define how to load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem. In the case that model_trained_path is true, that means that we have already trained the network so we load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def load_network(model_trained_path=None):\n",
    "  model = torchvision.models.resnet34(pretrained=True)\n",
    "  print(\"Adjusting the CNN for %s classes\" % num_classes)\n",
    "  model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "  #Define loss function\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  if (model_trained_path):\n",
    "    model.load_state_dict(torch.load(model_trained_path))\n",
    "  model = model.to(device)\n",
    "  return model,loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define finetuning util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "  \"\"\"\n",
    "  Train the model for one epoch.\n",
    "  \"\"\"\n",
    "  loss_epoch = 0 \n",
    "  start_time = time.time()\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run the model forward to compute scores and loss.\n",
    "    scores = model(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "    loss_epoch = loss_epoch + loss.item()\n",
    "    # Run the model backward and take a step using the optimizer.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50== 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "  return loss_epoch\n",
    "\n",
    "def make_preds(model, loader, device):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  # Set the model to eval mode\n",
    "  model.eval()\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  for x, y in loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    # Run the model forward, and compare the argmax score with the ground-truth\n",
    "    # category.\n",
    "    output = model(x)\n",
    "    predicted = output.argmax(1)\n",
    "    y_true.extend(y.cpu().numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "  return y_true,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Define the finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finetune(model,loss_fn,train_loader,device):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "  #First phase of finetuning\n",
    "  for epoch in range(num_epochs_ft1):\n",
    "    # Run an epoch over the training data.\n",
    "    print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "    loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "    # Check accuracy on the train and val sets.\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ft1}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "  #Allow updating all the weights in the second phase\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "  #Lower learning rate this time\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "  # Train the entire model for a few more epochs, checking accuracy on the\n",
    "  # train sets after each epoch.\n",
    "  for epoch in range(num_epochs_ft2):\n",
    "    print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "    loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ft2}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "    \n",
    "  torch.save(model.state_dict(), \"model.pt\")\n",
    "  print(\"Performing final validation in test examples...\")\n",
    "  y_true,y_pred = make_preds(model, val_loader, device)\n",
    "  return y_true,y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute everything and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(40,40))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Starting process...')\n",
    "model,loss_fn = load_network()\n",
    "y_true,y_pred = finetune(model,loss_fn,train_loader,device)\n",
    "cm=confusion_matrix(y_true, y_pred)\n",
    "labelswithexamples=np.union1d(np.unique(y_true),np.unique(y_pred))\n",
    "labelswithexamples_names = np.array(list(val_dset.class_to_idx.keys()))[labelswithexamples]\n",
    "plot_confusion_matrix(cm=cm,target_names=labelswithexamples_names,normalize=False)\n",
    "print(classification_report(y_true, y_pred,target_names=labelswithexamples_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the CNN for 51 classes\n"
     ]
    }
   ],
   "source": [
    "model,_=load_network('model.pt')\n",
    "y_true,y_pred = make_preds(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
